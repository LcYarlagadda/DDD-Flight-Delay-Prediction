{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection"
      ],
      "metadata": {
        "id": "53CNUAJAEJyU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üßæ IST History - Station List\n",
        "\n",
        "A station list to be used with the data files, showing the names and locations for each station.\n",
        "\n",
        "- **USAF**: Air Force station ID. May contain a letter in the first position.  \n",
        "- **WBAN**: NCDC WBAN number  \n",
        "- **CTRY**: FIPS country ID  \n",
        "- **ST**: State for US stations  \n",
        "- **ICAO**: ICAO ID  \n",
        "- **LAT**: Latitude in thousandths of decimal degrees  \n",
        "- **LON**: Longitude in thousandths of decimal degrees  \n",
        "- **ELEV**: Elevation in meters  \n",
        "- **BEGIN**: Beginning Period Of Record (YYYYMMDD). May have reporting gaps.  \n",
        "- **END**: Ending Period Of Record (YYYYMMDD). May have reporting gaps.  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#### üå¶Ô∏è DailyWeather - Global Surface Summary of the Day\n",
        "\n",
        "Produced by the National Climatic Data Center (NCDC), Asheville, NC.\n",
        "\n",
        "| Field   | Position    | Type   | Description |\n",
        "|---------|-------------|--------|-------------|\n",
        "| STN---  | 1‚Äì6         | Int    | Station number (WMO/DATSAV3 number) |\n",
        "| WBAN    | 8‚Äì12        | Int    | WBAN number (\"Weather Bureau Air Force Navy- Missing: 99999\") |\n",
        "| YEAR    | 15‚Äì18       | Int    | The year |\n",
        "| MODA    | 19‚Äì22       | Int    | Month and day |\n",
        "| TEMP    | 25‚Äì30       | Real   | Mean temperature (¬∞F to tenths) ‚Äî Missing: 9999.9 |\n",
        "| Count   | 32‚Äì33       | Int    | Obs count for mean temperature |\n",
        "| DEWP    | 36‚Äì41       | Real   | Mean dew point (¬∞F to tenths) ‚Äî Missing: 9999.9 |\n",
        "| Count   | 43‚Äì44       | Int    | Obs count for dew point |\n",
        "| SLP     | 47‚Äì52       | Real   | Mean sea-level pressure (mb) ‚Äî Missing: 9999.9 |\n",
        "| Count   | 54‚Äì55       | Int    | Obs count for sea-level pressure |\n",
        "| STP     | 58‚Äì63       | Real   | Mean station pressure (mb) ‚Äî Missing: 9999.9 |\n",
        "| Count   | 65‚Äì66       | Int    | Obs count for station pressure |\n",
        "| VISIB   | 69‚Äì73       | Real   | Mean visibility (miles) ‚Äî Missing: 999.9 |\n",
        "| Count   | 75‚Äì76       | Int    | Obs count for visibility |\n",
        "| WDSP    | 79‚Äì83       | Real   | Mean wind speed (knots) ‚Äî Missing: 999.9 |\n",
        "| Count   | 85‚Äì86       | Int    | Obs count for wind speed |\n",
        "| MXSPD   | 89‚Äì93       | Real   | Max sustained wind speed (knots) ‚Äî Missing: 999.9 |\n",
        "| GUST    | 96‚Äì100      | Real   | Max wind gust (knots) ‚Äî Missing: 999.9 |\n",
        "| MAX     | 103‚Äì108     | Real   | Max temperature (¬∞F) ‚Äî Missing: 9999.9 |\n",
        "| Flag    | 109         | Char   | Blank = from max report; * = derived from hourly |\n",
        "| MIN     | 111‚Äì116     | Real   | Min temperature (¬∞F) ‚Äî Missing: 9999.9 |\n",
        "| Flag    | 117         | Char   | Same rules as MAX |\n",
        "| PRCP    | 119‚Äì123     | Real   | Precipitation (inches) ‚Äî Missing: 99.99 |\n",
        "| Flag    | 124         | Char   | A‚ÄìI, source info for precipitation |\n",
        "| SNDP    | 126‚Äì130     | Real   | Snow depth (inches) ‚Äî Missing: 999.9 |\n",
        "| FRSHTT  | 133‚Äì138     | Int    | Flags for Fog, Rain, Snow, Hail, Thunder, Tornado (1 = yes, 0 = no) |\n",
        "\n"
      ],
      "metadata": {
        "id": "epkLlq3P_04w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ],
      "metadata": {
        "id": "yAdhKVudEPO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take only US stations from ISD History"
      ],
      "metadata": {
        "id": "_K_nkAweSVyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/ISD_History.csv')\n",
        "print(\"Total number of weather stations in ISD History\", len(df))\n",
        "\n",
        "# Filter for rows with COUNTRY == 'US', non-null ICAO, WBAN !=9999\n",
        "filtered_df = df[(df['CTRY'] == 'US') & (df['ICAO'].notna()) & (df['WBAN'] != 9999)]\n",
        "print(\"Number of US weather stations in ISD History\", len(filtered_df))\n",
        "\n",
        "# Save the filtered DataFrame to a new CSV file\n",
        "filtered_df.to_csv('ISD_History_US.csv', index=False)\n",
        "filtered_df.head()"
      ],
      "metadata": {
        "id": "6vIL7dTh1eoH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "12a15d8a-8d9f-49a8-e6b2-4f7524ac3a6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of weather stations in ISD History 29775\n",
            "Number of US weather stations in ISD History 5080\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         USAF   WBAN                  STATION NAME CTRY STATE  ICAO     LAT  \\\n",
              "14469  690020  93218  JOLON HUNTER LIGGETT MIL RES   US    CA  KHGT  36.000   \n",
              "14470  690020  99999  JOLON HUNTER LIGGETT MIL RES   US    CA  KHGT  36.000   \n",
              "14471  690070  93217                 FRITZSCHE AAF   US    CA  KOAR  36.683   \n",
              "14478  690140  93101                  EL TORO MCAS   US    CA  KNZJ  33.667   \n",
              "14479  690150  93121             TWENTY NINE PALMS   US    CA  KNXP  34.300   \n",
              "\n",
              "           LON  ELEV(M)     BEGIN       END  \n",
              "14469 -121.233    317.0  19640715  19970401  \n",
              "14470 -121.233    317.0  20030702  20030801  \n",
              "14471 -121.767     43.0  19600404  19930831  \n",
              "14478 -117.733    116.7  19890101  19990630  \n",
              "14479 -116.167    625.1  19900102  20190416  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-78f23f48-3d8a-4609-9387-080b4786b1f3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>USAF</th>\n",
              "      <th>WBAN</th>\n",
              "      <th>STATION NAME</th>\n",
              "      <th>CTRY</th>\n",
              "      <th>STATE</th>\n",
              "      <th>ICAO</th>\n",
              "      <th>LAT</th>\n",
              "      <th>LON</th>\n",
              "      <th>ELEV(M)</th>\n",
              "      <th>BEGIN</th>\n",
              "      <th>END</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14469</th>\n",
              "      <td>690020</td>\n",
              "      <td>93218</td>\n",
              "      <td>JOLON HUNTER LIGGETT MIL RES</td>\n",
              "      <td>US</td>\n",
              "      <td>CA</td>\n",
              "      <td>KHGT</td>\n",
              "      <td>36.000</td>\n",
              "      <td>-121.233</td>\n",
              "      <td>317.0</td>\n",
              "      <td>19640715</td>\n",
              "      <td>19970401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14470</th>\n",
              "      <td>690020</td>\n",
              "      <td>99999</td>\n",
              "      <td>JOLON HUNTER LIGGETT MIL RES</td>\n",
              "      <td>US</td>\n",
              "      <td>CA</td>\n",
              "      <td>KHGT</td>\n",
              "      <td>36.000</td>\n",
              "      <td>-121.233</td>\n",
              "      <td>317.0</td>\n",
              "      <td>20030702</td>\n",
              "      <td>20030801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14471</th>\n",
              "      <td>690070</td>\n",
              "      <td>93217</td>\n",
              "      <td>FRITZSCHE AAF</td>\n",
              "      <td>US</td>\n",
              "      <td>CA</td>\n",
              "      <td>KOAR</td>\n",
              "      <td>36.683</td>\n",
              "      <td>-121.767</td>\n",
              "      <td>43.0</td>\n",
              "      <td>19600404</td>\n",
              "      <td>19930831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14478</th>\n",
              "      <td>690140</td>\n",
              "      <td>93101</td>\n",
              "      <td>EL TORO MCAS</td>\n",
              "      <td>US</td>\n",
              "      <td>CA</td>\n",
              "      <td>KNZJ</td>\n",
              "      <td>33.667</td>\n",
              "      <td>-117.733</td>\n",
              "      <td>116.7</td>\n",
              "      <td>19890101</td>\n",
              "      <td>19990630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14479</th>\n",
              "      <td>690150</td>\n",
              "      <td>93121</td>\n",
              "      <td>TWENTY NINE PALMS</td>\n",
              "      <td>US</td>\n",
              "      <td>CA</td>\n",
              "      <td>KNXP</td>\n",
              "      <td>34.300</td>\n",
              "      <td>-116.167</td>\n",
              "      <td>625.1</td>\n",
              "      <td>19900102</td>\n",
              "      <td>20190416</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-78f23f48-3d8a-4609-9387-080b4786b1f3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-78f23f48-3d8a-4609-9387-080b4786b1f3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-78f23f48-3d8a-4609-9387-080b4786b1f3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4f116a88-d867-4663-bf08-38a4fa3555f7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4f116a88-d867-4663-bf08-38a4fa3555f7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4f116a88-d867-4663-bf08-38a4fa3555f7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "filtered_df",
              "summary": "{\n  \"name\": \"filtered_df\",\n  \"rows\": 5080,\n  \"fields\": [\n    {\n      \"column\": \"USAF\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2727,\n        \"samples\": [\n          \"724033\",\n          \"722666\",\n          \"725107\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"WBAN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 41635,\n        \"min\": 102,\n        \"max\": 99999,\n        \"num_unique_values\": 2322,\n        \"samples\": [\n          23090,\n          3102,\n          94896\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"STATION NAME\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4737,\n        \"samples\": [\n          \"NAES/MAXFIELD FIELD\",\n          \"MUNISING LAKESHORE\",\n          \"HAMILTON MUNICIPAL AIRPORT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CTRY\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"US\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"STATE\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 53,\n        \"samples\": [\n          \"MT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ICAO\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2650,\n        \"samples\": [\n          \"KORE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LAT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.69554655899085,\n        \"min\": -32.133,\n        \"max\": 71.333,\n        \"num_unique_values\": 2931,\n        \"samples\": [\n          31.583\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LON\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 23.044298272387348,\n        \"min\": -176.65,\n        \"max\": 174.1,\n        \"num_unique_values\": 3344,\n        \"samples\": [\n          -71.799\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ELEV(M)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 543.1936454407904,\n        \"min\": -999.0,\n        \"max\": 4113.3,\n        \"num_unique_values\": 2354,\n        \"samples\": [\n          1707.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BEGIN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 237584,\n        \"min\": 19310101,\n        \"max\": 20190307,\n        \"num_unique_values\": 1349,\n        \"samples\": [\n          20130514\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"END\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 184351,\n        \"min\": 19361231,\n        \"max\": 20190417,\n        \"num_unique_values\": 683,\n        \"samples\": [\n          19951231\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract data for years 1998-2002 from the tar files"
      ],
      "metadata": {
        "id": "ypJenPNC1fWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "import gzip\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "# The paths to your yearly tar files\n",
        "tar_years = [1998, 1999]\n",
        "# tar_years = [2000, 2001, 2002]\n",
        "\n",
        "# Get Columns based on the file format\n",
        "colspecs = [\n",
        "    (0, 6),    # STN--- (USAF weather station ID)\n",
        "    (7, 12),   # WBAN\n",
        "    (13, 18),  # YEAR\n",
        "    (19, 22),  # MODA (Month/Day)\n",
        "    (24, 30),  # TEMP (mean temperature)\n",
        "    (30, 33),  # TEMP count\n",
        "    (35, 41),  # DEWP (mean dewpoint)\n",
        "    (41, 44),  # DEWP count\n",
        "    (46, 52),  # SLP (mean sea level pressure)\n",
        "    (52, 55),  # SLP count\n",
        "    (57, 63),  # STP (mean station pressure)\n",
        "    (63, 66),  # STP count\n",
        "    (68, 74),  # VISIB (mean visibility)\n",
        "    (74, 77),  # VISIB count\n",
        "    (79, 85),  # WDSP (mean wind speed)\n",
        "    (85, 88),  # WDSP count\n",
        "    (90, 96),  # MXSPD (max wind speed)\n",
        "    (96, 101), # GUST (max gust speed)\n",
        "    (103, 109),# MAX (max temperature)\n",
        "    (110, 116),# MIN (min temperature)\n",
        "    (118, 123),# PRCP (precipitation)\n",
        "    (125, 130),# SNDP (snow depth)\n",
        "    (132, 138) # FRSHTT (indicator flags)\n",
        "]\n",
        "\n",
        "# Define column names corresponding to the colspecs above\n",
        "col_names = [\n",
        "    \"STN\", \"WBAN\", \"YEAR\", \"MODA\",\n",
        "    \"TEMP\", \"TEMP_Count\",\n",
        "    \"DEWP\", \"DEWP_Count\",\n",
        "    \"SLP\",  \"SLP_Count\",\n",
        "    \"STP\",  \"STP_Count\",\n",
        "    \"VISIB\",\"VISIB_Count\",\n",
        "    \"WDSP\", \"WDSP_Count\",\n",
        "    \"MXSPD\",\"GUST\",\n",
        "    \"MAX\",  \"MIN\",\n",
        "    \"PRCP\", \"SNDP\",\n",
        "    \"FRSHTT\"\n",
        "]\n",
        "\n",
        "all_dfs = []\n",
        "\n",
        "for year in tar_years:\n",
        "    tar_path = f\"/content/gsod_{year}.tar\"\n",
        "    print(f\"Processing: {tar_path}\")\n",
        "\n",
        "    with tarfile.open(tar_path, \"r\") as year_tar:\n",
        "        # Loop through each member in the main yearly tar\n",
        "        for member in year_tar.getmembers():\n",
        "            # We only care about .op.gz files\n",
        "            if member.isfile() and member.name.endswith(\".op.gz\"):\n",
        "                file_obj = year_tar.extractfile(member)\n",
        "\n",
        "                if file_obj is not None:\n",
        "                    # Decompress the .gz in memory\n",
        "                    with gzip.open(file_obj, \"rt\", errors=\"replace\") as gz:\n",
        "                        try:\n",
        "                            # Read the fixed-width file directly with pandas\n",
        "                            df = pd.read_fwf(\n",
        "                                gz,\n",
        "                                colspecs=colspecs,\n",
        "                                names=col_names,\n",
        "                                header=None,\n",
        "                                skiprows=1  # Remove if no header lines\n",
        "                            )\n",
        "                            all_dfs.append(df)\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error parsing {member.name} from {tar_path}: {e}\")\n",
        "\n",
        "# Combine everything\n",
        "final_df = pd.concat(all_dfs, ignore_index=True)\n",
        "\n",
        "# Save to CSV\n",
        "output_csv = \"gsod_merged_1998_1999.csv\"\n",
        "# output_csv = \"gsod_merged_2000_2002.csv\"\n",
        "final_df.to_csv(output_csv, index=False)\n",
        "print(f\"{tar_years} merged into: {output_csv}\")"
      ],
      "metadata": {
        "id": "agP0vXHK1lGh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b65db20-ca11-41ef-f0b9-3b51a686ba6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: /content/gsod_1998.tar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keep only april weather data for all years"
      ],
      "metadata": {
        "id": "1zRiwuf51l_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Concatenate all years\n",
        "df1 = pd.read_csv('gsod_merged_1998_1999.csv')\n",
        "df2 = pd.read_csv('gsod_merged_2000_2002.csv')\n",
        "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
        "print(\"Total records in the weather dataset\", len(merged_df))\n",
        "\n",
        "# Optional: filter weather to April only\n",
        "merged_df[\"MODA\"] = merged_df[\"MODA\"].astype(int)\n",
        "\n",
        "# Filter for April (month = 4) and years 1998 to 2002\n",
        "merged_df = merged_df[\n",
        "    (merged_df[\"YEAR\"].between(1998, 2002)) &\n",
        "    ((merged_df[\"MODA\"] // 100) == 4)\n",
        "]\n",
        "\n",
        "print(\"Total records in the weather dataset for april\", len(merged_df))\n",
        "\n",
        "# Drop rows where WBAN column equals 99999, Invalid stations\n",
        "merged_df = merged_df[merged_df['WBAN'] != 99999]\n",
        "print(\"Total records in the weather dataset for correct weather stations\", len(merged_df))\n",
        "\n",
        "merged_df.to_csv(\"/content/Daily_Weather_April.csv\", index=False)\n",
        "merged_df.head()"
      ],
      "metadata": {
        "id": "Gilw3uj81msh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace all missing values with NA"
      ],
      "metadata": {
        "id": "FidQEgsv12aZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv(\"/content/Daily_Weather_April.csv\")\n",
        "\n",
        "# Replace known missing flags with NaN\n",
        "missing_value_map = {\n",
        "    \"TEMP\": 9999.9,\n",
        "    \"DEWP\": 9999.9,\n",
        "    \"SLP\": 9999.9,\n",
        "    \"STP\": 9999.9,\n",
        "    \"VISIB\": 999.9,\n",
        "    \"WDSP\": 999.9,\n",
        "    \"MXSPD\": 999.9,\n",
        "    \"GUST\": 999.9,\n",
        "    \"MAX\": 9999.9,\n",
        "    \"MIN\": 9999.9,\n",
        "    \"PRCP\": 99.99,\n",
        "    \"SNDP\": 999.9\n",
        "}\n",
        "\n",
        "# Convert and clean\n",
        "for col, missing_val in missing_value_map.items():\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "        df[col] = df[col].replace(missing_val, pd.NA)\n",
        "\n",
        "# # Fill missing values with column mean\n",
        "# df.fillna(df.mean(numeric_only=True), inplace=True)\n",
        "\n",
        "# Save cleaned data\n",
        "df.to_csv(\"/content/Cleaned_Daily_Weather.csv\", index=False)\n",
        "print(\"Cleaned dataset saved as cleaned_daily_weather.csv\")\n"
      ],
      "metadata": {
        "id": "kxysV0yo1zie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ae455ed-defe-4c19-9193-35cbd171f081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned dataset saved as cleaned_daily_weather.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Integration"
      ],
      "metadata": {
        "id": "XO3b8U1L18-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge Daily weather and ist history based on WBAN for getting ICAO code, for finding flight IATA"
      ],
      "metadata": {
        "id": "2JdyM2fm18O4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the CSV files\n",
        "weather_df = pd.read_csv(\"/content/Cleaned_Daily_Weather.csv\", skipinitialspace=True)\n",
        "stations_df = pd.read_csv(\"/content/ISD_History_US.csv\", skipinitialspace=True)\n",
        "\n",
        "# Strip whitespaces from column names\n",
        "stations_df.columns = stations_df.columns.str.strip()\n",
        "weather_df.columns = weather_df.columns.str.strip()\n",
        "\n",
        "# Ensure WBANs are strings to match properly\n",
        "weather_df['WBAN'] = weather_df['WBAN'].astype(str).str.strip()\n",
        "stations_df['WBAN'] = stations_df['WBAN'].astype(str).str.strip()\n",
        "\n",
        "# Create a dictionary for quick lookup of station rows by WBAN\n",
        "station_lookup = stations_df.drop_duplicates(subset=\"WBAN\").set_index(\"WBAN\").to_dict(orient='index')\n",
        "\n",
        "# Prepare merged output list\n",
        "merged_records = []\n",
        "\n",
        "# Iterate over weather data\n",
        "for _, weather_row in tqdm(weather_df.iterrows(), total=len(weather_df), desc=\"Merging ICAO info\", ncols=100):\n",
        "    wban = weather_row['WBAN']\n",
        "    station_info = station_lookup.get(wban, None)\n",
        "\n",
        "    if station_info:\n",
        "        combined_row = weather_row.to_dict()\n",
        "        combined_row.update(station_info)\n",
        "        merged_records.append(combined_row)\n",
        "    else:\n",
        "        # Keep the original weather row if no match found\n",
        "        merged_records.append(weather_row.to_dict())\n",
        "\n",
        "# Create final DataFrame\n",
        "final_df = pd.DataFrame(merged_records)\n",
        "\n",
        "# Save the merged file\n",
        "final_df.to_csv(\"/content/Daily_Weather_ICAO.csv\", index=False)\n",
        "print(f\"Successfully merged {len(final_df)} records and saved to Daily_Weather_ICAO.csv\")\n"
      ],
      "metadata": {
        "id": "0XzLr5TK1_c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0f70028-f3aa-4631-9953-5f312a9d5fb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Merging ICAO info: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 88112/88112 [00:05<00:00, 16057.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully merged 88112 records and saved to Daily_Weather_ICAO.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use airportsdata package to convert ICAO into IATA code"
      ],
      "metadata": {
        "id": "T8wODQiB2BmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install airportsdata"
      ],
      "metadata": {
        "id": "3hYEX6pE2FG6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6990793-39f9-44fe-a3b7-43889658d3e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting airportsdata\n",
            "  Downloading airportsdata-20250224-py3-none-any.whl.metadata (9.0 kB)\n",
            "Downloading airportsdata-20250224-py3-none-any.whl (913 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m913.7/913.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: airportsdata\n",
            "Successfully installed airportsdata-20250224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from airportsdata import load\n",
        "\n",
        "# Load global airport data using ICAO codes\n",
        "airports = load('ICAO')\n",
        "\n",
        "# Read the merged weather-station data\n",
        "df = pd.read_csv(\"/content/Daily_Weather_ICAO.csv\")\n",
        "\n",
        "# Map ICAO to IATA using the airport database\n",
        "def icao_to_iata(icao_code):\n",
        "    airport = airports.get(icao_code)\n",
        "    return airport['iata'] if airport else None\n",
        "\n",
        "# Add IATA column\n",
        "df['IATA'] = df['ICAO'].apply(icao_to_iata)\n",
        "print(f\"Total records: {len(df)}\")\n",
        "\n",
        "# Drop rows where IATA is missing (null)\n",
        "df_cleaned = df.dropna(subset=['IATA'])\n",
        "\n",
        "# Print the count of remaining records\n",
        "print(f\"Records with valid IATA code: {len(df_cleaned)}\")\n",
        "\n",
        "# Save the cleaned dataset (optional)\n",
        "df_cleaned.to_csv(\"/content/Daily_Weather_IATA.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "BlGWnJ3T2G_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1ec39cf-0d94-4b5a-ec7c-ebf60ecaedbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total records: 88112\n",
            "Records with valid IATA code: 79767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a common column in weatherdata for merging weather and airport data: DDMMYYYYYIATA"
      ],
      "metadata": {
        "id": "8pCG6OFn2MPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the weather dataset\n",
        "weather_df = pd.read_csv(\"/content/Daily_Weather_IATA.csv\")\n",
        "\n",
        "def convert_to_ddmmyyyy_iata(row):\n",
        "    # Extract year, MODA, and IATA\n",
        "    year = row['YEAR']\n",
        "    mod = str(row['MODA']).zfill(3)  # Ensure MODA is always a 3-digit string\n",
        "    iata = row['IATA']\n",
        "\n",
        "    # Extract month (first digit) and day (next two digits)\n",
        "    month = \"0\"+mod[0]  # First digit as month\n",
        "    day = mod[1:3]  # Next two digits as day\n",
        "\n",
        "    # Create the formatted string as DDMMYYYYIATA\n",
        "    formatted_date = f\"{day}{month}{year}{iata}\"\n",
        "\n",
        "    return formatted_date\n",
        "\n",
        "# Apply the conversion to each row and create the new column\n",
        "weather_df['DDMMYYYYIATA'] = weather_df.apply(convert_to_ddmmyyyy_iata, axis=1)\n",
        "\n",
        "weather_df.drop_duplicates(subset=\"DDMMYYYYIATA\", inplace=True)\n",
        "print(len(weather_df))\n",
        "\n",
        "# Display the result (first few rows)\n",
        "print(weather_df[['YEAR', 'MODA', 'IATA', 'DDMMYYYYIATA']].head())\n",
        "\n",
        "# Optional: Save the updated dataset\n",
        "weather_df.to_csv(\"/content/Final_Weather_Data.csv\", index=False)"
      ],
      "metadata": {
        "id": "WKZK3s2w2PKm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c6c41c7-8edb-4577-e358-d96f8907b2d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "77168\n",
            "   YEAR  MODA IATA DDMMYYYYIATA\n",
            "0  1998   401  BIX  01041998BIX\n",
            "1  1998   402  BIX  02041998BIX\n",
            "2  1998   403  BIX  03041998BIX\n",
            "3  1998   404  BIX  04041998BIX\n",
            "4  1998   405  BIX  05041998BIX\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See missing value columns in the weather data"
      ],
      "metadata": {
        "id": "28MBTVWB2Soe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the weather data\n",
        "df = pd.read_csv('/content/Final_Weather_Data.csv')  # replace with your file path\n",
        "\n",
        "# Calculate missing data percentage\n",
        "missing_percentages = df.isnull().mean() * 100\n",
        "\n",
        "# Filter columns with more than 30% missing data\n",
        "high_missing_cols = missing_percentages[missing_percentages > 30]\n",
        "\n",
        "print(\"Weather columns with more than 30% missing values:\")\n",
        "print(high_missing_cols.sort_values(ascending=False))\n"
      ],
      "metadata": {
        "id": "P35Ay4Qa2Tt7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd89d6f0-23d6-47e6-fb16-503fe6eb1249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weather columns with more than 30% missing values:\n",
            "WDSP     98.306293\n",
            "SNDP     97.186658\n",
            "STP      83.813239\n",
            "MAX      43.869220\n",
            "MXSPD    38.234761\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop irrelevant and columns with more than 30% missing values"
      ],
      "metadata": {
        "id": "2xP0NGEA2X8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of columns to drop\n",
        "columns_to_drop = [\n",
        "    'WBAN','WDSP', 'SNDP', 'STP', 'MAX', 'MXSPD', 'STN', 'YEAR', 'MODA',\n",
        "    'TEMP_Count', 'DEWP_Count', 'SLP_Count', 'STP_Count',\n",
        "    'VISIB_Count', 'WDSP_Count', 'MIN', 'USAF', 'STATION NAME',\n",
        "    'CTRY', 'STATE', 'ICAO', 'LAT', 'LON', 'ELEV(M)', 'BEGIN', 'END', 'IATA'\n",
        "]\n",
        "\n",
        "# Drop the columns from the DataFrame\n",
        "df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
        "\n",
        "# Confirm\n",
        "print(f\"Dropped {len(columns_to_drop)} columns. Remaining columns: {df.shape[1]}\")"
      ],
      "metadata": {
        "id": "IuHn1NC22YkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82793b83-c587-4d68-f0d7-d511612c823c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropped 27 columns. Remaining columns: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check for missing values in each column"
      ],
      "metadata": {
        "id": "Eq-OjqHa2aym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display count of missing (NaN) values for each column\n",
        "missing_data = df.isna().sum()\n",
        "\n",
        "# Display the columns that have missing data and their counts\n",
        "print(\"Missing values in each column:\")\n",
        "print(missing_data[missing_data > 0])"
      ],
      "metadata": {
        "id": "PsdYqynI2dKz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "783ae136-33ef-4b37-ce47-0c9b68cd3965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in each column:\n",
            "DEWP      319\n",
            "SLP      5845\n",
            "VISIB      54\n",
            "PRCP      887\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill missing values with mean"
      ],
      "metadata": {
        "id": "COrzErus2gxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['VISIB'] = df['VISIB'].fillna(df['VISIB'].mean())\n",
        "df['PRCP'] = df['PRCP'].fillna(0)\n",
        "df['SLP'] = df['SLP'].fillna(df['SLP'].mean())\n",
        "df['DEWP'] = df['DEWP'].fillna(df['DEWP'].mean())\n",
        "df['TEMP'] = df['TEMP'].fillna(df['TEMP'].mean())\n",
        "df['GUST'] = df['GUST'].fillna(df['GUST'].mean())\n",
        "df['FRSHTT'] = df['FRSHTT'].fillna(0)"
      ],
      "metadata": {
        "id": "sEcSYGGO2hJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display count of missing (NaN) values for each column\n",
        "missing_data = df.isna().sum()\n",
        "\n",
        "# Display the columns that have missing data and their counts\n",
        "print(\"Missing values in each column:\")\n",
        "print(missing_data[missing_data > 0])"
      ],
      "metadata": {
        "id": "iZO4Ejrq2oKU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3005aae6-529c-4094-de68-e046582ee5d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in each column:\n",
            "Series([], dtype: int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check for outliers"
      ],
      "metadata": {
        "id": "2F0K4npn2rMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Filter numeric columns starting with 'WEATHER_'\n",
        "weather_numeric_cols = [col for col in df.columns if df[col].dtype in ['float64', 'int64']]\n",
        "\n",
        "# Dictionary to store summary\n",
        "weather_outlier_summary = []\n",
        "\n",
        "# Loop through each weather column\n",
        "for col in weather_numeric_cols:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
        "    outlier_count = outliers.shape[0]\n",
        "    total_count = df[col].count()\n",
        "\n",
        "    if total_count == 0:\n",
        "        outlier_pct = 0\n",
        "    else:\n",
        "        outlier_pct = (outlier_count / total_count) * 100\n",
        "\n",
        "    weather_outlier_summary.append({\n",
        "        'Column': col,\n",
        "        'Q1': round(Q1, 2),\n",
        "        'Q3': round(Q3, 2),\n",
        "        'IQR': round(IQR, 2),\n",
        "        'Lower Bound': round(lower_bound, 2),\n",
        "        'Upper Bound': round(upper_bound, 2),\n",
        "        'Outlier Count': outlier_count,\n",
        "        'Total Count': total_count,\n",
        "        'Outlier %': round(outlier_pct, 2)\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame and sort\n",
        "outlier_df = pd.DataFrame(weather_outlier_summary).sort_values(by='Outlier %', ascending=False)\n",
        "\n",
        "# Display the formatted table\n",
        "print(outlier_df.to_string(index=False))\n"
      ],
      "metadata": {
        "id": "Ewpj0TAm2q1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e90bb260-6f09-4bf2-9554-b40c506fb246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column     Q1        Q3       IQR  Lower Bound  Upper Bound  Outlier Count  Total Count  Outlier %\n",
            "  PRCP    0.0      0.02      0.02        -0.03         0.05          15046        77168      19.50\n",
            " VISIB    8.3      9.90      1.60         5.90        12.30          11930        77168      15.46\n",
            "   SLP 1011.5   1019.30      7.80       999.80      1031.00           2149        77168       2.78\n",
            "  TEMP   45.0     64.20     19.20        16.20        93.00            847        77168       1.10\n",
            "  DEWP   29.8     50.50     20.70        -1.25        81.55            384        77168       0.50\n",
            "  GUST   21.0     99.90     78.90       -97.35       218.25              0        77168       0.00\n",
            "FRSHTT    0.0 100000.00 100000.00   -150000.00    250000.00              0        77168       0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a weather condition column based on last three days weather"
      ],
      "metadata": {
        "id": "dR8Kj3CI2t6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df['FRSHTT'] = df['FRSHTT'].fillna(0).astype(int).astype(str).str.zfill(6)\n",
        "\n",
        "# Step 1: Clean and expand FRSHTT\n",
        "frshtt_cols = ['FOG', 'RAIN', 'SNOW', 'HAIL', 'THUNDER', 'TORNADO']\n",
        "for i, col in enumerate(frshtt_cols):\n",
        "    df[f'WEATHER_{col}'] = df['FRSHTT'].str[i].astype(int)\n",
        "\n",
        "# Step 2: Extract date and IATA\n",
        "df['DATE'] = pd.to_datetime(df['DDMMYYYYIATA'].str[:8], format='%d%m%Y', errors='coerce')\n",
        "df['IATA'] = df['DDMMYYYYIATA'].str[8:]\n",
        "\n",
        "# Step 3: Sort the data\n",
        "df = df.sort_values(by=['IATA', 'DATE'])\n",
        "\n",
        "# Step 4: Rolling logic with fallback to fewer days or today's value\n",
        "for col in frshtt_cols:\n",
        "    # Get past 3 days (excluding today)\n",
        "    rolled = (\n",
        "        df\n",
        "        .groupby('IATA')[f'WEATHER_{col}']\n",
        "        .apply(lambda x: x.shift(1).rolling(window=3, min_periods=1).mean())\n",
        "        .reset_index(level=0, drop=True)\n",
        "    )\n",
        "\n",
        "    # Fill missing rolling values with fewer day fallback or today's value\n",
        "    df[f'AVG3_{col}'] = rolled.fillna(df[f'WEATHER_{col}'])\n",
        "\n",
        "# Step 5: Classify weather condition\n",
        "def classify_weather(row):\n",
        "    if row['AVG3_TORNADO'] >= 0.2 or row['AVG3_HAIL'] >= 0.2:\n",
        "        return 'stormy'\n",
        "    elif row['AVG3_THUNDER'] >= 0.2:\n",
        "        return 'thunderstorm'\n",
        "    elif row['AVG3_RAIN'] >= 0.3:\n",
        "        return 'rainy'\n",
        "    elif row['AVG3_SNOW'] >= 0.2:\n",
        "        return 'snowy'\n",
        "    elif row['AVG3_FOG'] >= 0.3:\n",
        "        return 'foggy'\n",
        "    else:\n",
        "        return 'clear'\n",
        "\n",
        "df['WEATHER_CONDITION'] = df.apply(classify_weather, axis=1)\n",
        "\n",
        "# Step 6: Cleanup intermediate columns\n",
        "to_drop = [f'WEATHER_{col}' for col in frshtt_cols] + [f'AVG3_{col}' for col in frshtt_cols] + ['DATE', 'IATA']\n",
        "df.drop(columns=to_drop, inplace=True)\n",
        "\n",
        "# Done. Preview result\n",
        "df.head(10)\n"
      ],
      "metadata": {
        "id": "oRdreF5g2yld",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "a66fd8c2-dcf5-4cf3-f44a-cf3ba1740729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       TEMP  DEWP     SLP      VISIB  GUST  PRCP  FRSHTT DDMMYYYYIATA  \\\n",
              "19041  70.1  67.9  1015.4   5.400000  99.9  0.29  010000  01041999AAF   \n",
              "19042  71.0  68.0  1016.3   6.300000  99.9  0.38  100000  02041999AAF   \n",
              "19043  68.1  65.7  1016.6   7.000000  99.9  0.01  100000  03041999AAF   \n",
              "19044  70.1  66.8  1017.2   4.500000  99.9  0.01  100000  04041999AAF   \n",
              "19045  72.1  67.6  1016.8   6.100000  99.9  0.01  100000  05041999AAF   \n",
              "19046  71.2  65.1  1019.6   7.600000  99.9  0.00  000000  06041999AAF   \n",
              "19047  70.9  66.4  1022.2   5.400000  99.9  0.01  000000  07041999AAF   \n",
              "19048  76.2  69.3  1019.4   6.900000  99.9  0.00  000000  08041999AAF   \n",
              "19049  76.1  69.6  1015.1   7.500000  23.9  0.00  000000  09041999AAF   \n",
              "19050  76.5  69.9  1013.4  10.204097  21.0  0.00  000000  10041999AAF   \n",
              "\n",
              "      WEATHER_CONDITION  \n",
              "19041             rainy  \n",
              "19042             rainy  \n",
              "19043             rainy  \n",
              "19044             rainy  \n",
              "19045             foggy  \n",
              "19046             foggy  \n",
              "19047             foggy  \n",
              "19048             foggy  \n",
              "19049             clear  \n",
              "19050             clear  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f65a412b-ebe6-465d-a561-c2e4cf3a51ab\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TEMP</th>\n",
              "      <th>DEWP</th>\n",
              "      <th>SLP</th>\n",
              "      <th>VISIB</th>\n",
              "      <th>GUST</th>\n",
              "      <th>PRCP</th>\n",
              "      <th>FRSHTT</th>\n",
              "      <th>DDMMYYYYIATA</th>\n",
              "      <th>WEATHER_CONDITION</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>19041</th>\n",
              "      <td>70.1</td>\n",
              "      <td>67.9</td>\n",
              "      <td>1015.4</td>\n",
              "      <td>5.400000</td>\n",
              "      <td>99.9</td>\n",
              "      <td>0.29</td>\n",
              "      <td>010000</td>\n",
              "      <td>01041999AAF</td>\n",
              "      <td>rainy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19042</th>\n",
              "      <td>71.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1016.3</td>\n",
              "      <td>6.300000</td>\n",
              "      <td>99.9</td>\n",
              "      <td>0.38</td>\n",
              "      <td>100000</td>\n",
              "      <td>02041999AAF</td>\n",
              "      <td>rainy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19043</th>\n",
              "      <td>68.1</td>\n",
              "      <td>65.7</td>\n",
              "      <td>1016.6</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>99.9</td>\n",
              "      <td>0.01</td>\n",
              "      <td>100000</td>\n",
              "      <td>03041999AAF</td>\n",
              "      <td>rainy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19044</th>\n",
              "      <td>70.1</td>\n",
              "      <td>66.8</td>\n",
              "      <td>1017.2</td>\n",
              "      <td>4.500000</td>\n",
              "      <td>99.9</td>\n",
              "      <td>0.01</td>\n",
              "      <td>100000</td>\n",
              "      <td>04041999AAF</td>\n",
              "      <td>rainy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19045</th>\n",
              "      <td>72.1</td>\n",
              "      <td>67.6</td>\n",
              "      <td>1016.8</td>\n",
              "      <td>6.100000</td>\n",
              "      <td>99.9</td>\n",
              "      <td>0.01</td>\n",
              "      <td>100000</td>\n",
              "      <td>05041999AAF</td>\n",
              "      <td>foggy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19046</th>\n",
              "      <td>71.2</td>\n",
              "      <td>65.1</td>\n",
              "      <td>1019.6</td>\n",
              "      <td>7.600000</td>\n",
              "      <td>99.9</td>\n",
              "      <td>0.00</td>\n",
              "      <td>000000</td>\n",
              "      <td>06041999AAF</td>\n",
              "      <td>foggy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19047</th>\n",
              "      <td>70.9</td>\n",
              "      <td>66.4</td>\n",
              "      <td>1022.2</td>\n",
              "      <td>5.400000</td>\n",
              "      <td>99.9</td>\n",
              "      <td>0.01</td>\n",
              "      <td>000000</td>\n",
              "      <td>07041999AAF</td>\n",
              "      <td>foggy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19048</th>\n",
              "      <td>76.2</td>\n",
              "      <td>69.3</td>\n",
              "      <td>1019.4</td>\n",
              "      <td>6.900000</td>\n",
              "      <td>99.9</td>\n",
              "      <td>0.00</td>\n",
              "      <td>000000</td>\n",
              "      <td>08041999AAF</td>\n",
              "      <td>foggy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19049</th>\n",
              "      <td>76.1</td>\n",
              "      <td>69.6</td>\n",
              "      <td>1015.1</td>\n",
              "      <td>7.500000</td>\n",
              "      <td>23.9</td>\n",
              "      <td>0.00</td>\n",
              "      <td>000000</td>\n",
              "      <td>09041999AAF</td>\n",
              "      <td>clear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19050</th>\n",
              "      <td>76.5</td>\n",
              "      <td>69.9</td>\n",
              "      <td>1013.4</td>\n",
              "      <td>10.204097</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>000000</td>\n",
              "      <td>10041999AAF</td>\n",
              "      <td>clear</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f65a412b-ebe6-465d-a561-c2e4cf3a51ab')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f65a412b-ebe6-465d-a561-c2e4cf3a51ab button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f65a412b-ebe6-465d-a561-c2e4cf3a51ab');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-240e0c23-fc7e-4fdf-8148-a4e78c54e122\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-240e0c23-fc7e-4fdf-8148-a4e78c54e122')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-240e0c23-fc7e-4fdf-8148-a4e78c54e122 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 77168,\n  \"fields\": [\n    {\n      \"column\": \"TEMP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13.90444331514686,\n        \"min\": -29.3,\n        \"max\": 89.4,\n        \"num_unique_values\": 987,\n        \"samples\": [\n          38.7,\n          61.0,\n          -2.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DEWP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14.900654697781144,\n        \"min\": -39.1,\n        \"max\": 77.1,\n        \"num_unique_values\": 944,\n        \"samples\": [\n          47.5,\n          35.8,\n          6.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SLP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.706081839783074,\n        \"min\": 971.3,\n        \"max\": 1044.4,\n        \"num_unique_values\": 584,\n        \"samples\": [\n          1031.9,\n          1030.8,\n          1009.8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"VISIB\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.337288923222889,\n        \"min\": 0.0,\n        \"max\": 72.1,\n        \"num_unique_values\": 532,\n        \"samples\": [\n          6.9,\n          38.4,\n          13.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"GUST\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 37.286414209410765,\n        \"min\": 11.1,\n        \"max\": 99.9,\n        \"num_unique_values\": 70,\n        \"samples\": [\n          34.0,\n          99.9,\n          12.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PRCP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.24955437657227167,\n        \"min\": 0.0,\n        \"max\": 6.91,\n        \"num_unique_values\": 311,\n        \"samples\": [\n          2.72,\n          0.08,\n          0.96\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FRSHTT\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 33,\n        \"samples\": [\n          \"011111\",\n          \"011110\",\n          \"010111\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DDMMYYYYIATA\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 77168,\n        \"samples\": [\n          \"06041999MLS\",\n          \"11041999HON\",\n          \"24041999FCA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"WEATHER_CONDITION\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"rainy\",\n          \"foggy\",\n          \"snowy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"/content/FinalizedWeather_Data.csv\", index=False)"
      ],
      "metadata": {
        "id": "oZoXomwm22dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a common column in flight data for merging weather and airport data: DDMMYYYYYIATA"
      ],
      "metadata": {
        "id": "jc9ZkrqI21mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the flight dataset\n",
        "flight_df = pd.read_csv(\"/content/df_cleaned.csv\")\n",
        "\n",
        "# Function to convert flight data into DDMMYYYYIATA format\n",
        "def convert_to_ddmmyyyy_iata_flight(row):\n",
        "    # Extract day, month, year, and origin airport code\n",
        "    day = str(row['DAY_OF_MONTH']).zfill(2)  # Ensure day is two digits\n",
        "    month = \"04\"  # Ensure month is two digits\n",
        "    year = str(row['YEAR'])  # Four digit year\n",
        "    origin = row['ORIGIN']  # Origin airport code\n",
        "\n",
        "    # Combine them into DDMMYYYYIATA format\n",
        "    formatted_date = f\"{day}{month}{year}{origin}\"\n",
        "\n",
        "    return formatted_date\n",
        "\n",
        "# Apply the conversion to each row and create the new column\n",
        "flight_df['DDMMYYYYIATA'] = flight_df.apply(convert_to_ddmmyyyy_iata_flight, axis=1)\n",
        "\n",
        "# Display the result (first few rows)\n",
        "print(flight_df[['YEAR', 'DAY_OF_MONTH', 'ORIGIN', 'DDMMYYYYIATA']].head())\n",
        "\n",
        "print(len(flight_df))\n",
        "# Optional: Save the updated dataset\n",
        "flight_df.to_csv(\"/content/FinalFlight_Data.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "rg9AmXGM2-7H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e6db8a0-aad2-44e0-96ae-0ab4bd9abf48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   YEAR  DAY_OF_MONTH ORIGIN DDMMYYYYIATA\n",
            "0  1998             1    ABQ  01041998ABQ\n",
            "1  1998             1    ABQ  01041998ABQ\n",
            "2  1998             1    ABQ  01041998ABQ\n",
            "3  1998             1    ABQ  01041998ABQ\n",
            "4  1998             1    ABQ  01041998ABQ\n",
            "2265248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge records based on DDMMYYYYYIATA"
      ],
      "metadata": {
        "id": "V7R7xkAC2-e2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Parameters ===\n",
        "start_index = 0\n",
        "chunk_size = 500_000\n",
        "output_prefix = \"/content/Merge_part\"\n",
        "\n",
        "# === Load datasets ===\n",
        "weather_df = pd.read_csv(\"/content/FinalizedWeather_Data.csv\")\n",
        "april_df = pd.read_csv(\"/content/FinalFlight_Data.csv\")\n",
        "print(\"Loaded datasets\")\n",
        "\n",
        "# === Rename weather columns to avoid collision ===\n",
        "weather_df = weather_df.add_prefix(\"WEATHER_\")\n",
        "weather_df.rename(columns={\"WEATHER_DDMMYYYYIATA\": \"DDMMYYYYIATA\"}, inplace=True)\n",
        "\n",
        "# === Sort and set index for faster matching ===\n",
        "weather_df.sort_values(by=\"DDMMYYYYIATA\", inplace=True)\n",
        "april_df.sort_values(by=\"DDMMYYYYIATA\", inplace=True)\n",
        "weather_df.set_index(\"DDMMYYYYIATA\", inplace=True)\n",
        "\n",
        "# === Merge process ===\n",
        "merged_chunk = []\n",
        "file_index = (start_index // chunk_size) + 1\n",
        "\n",
        "for i in tqdm(range(start_index, len(april_df)), desc=\"Merging\", ncols=100):\n",
        "    flight_row = april_df.iloc[i]\n",
        "    flight_key = flight_row[\"DDMMYYYYIATA\"]\n",
        "\n",
        "    # Find matching weather record\n",
        "    if flight_key in weather_df.index:\n",
        "        weather_row = weather_df.loc[flight_key]\n",
        "        if isinstance(weather_row, pd.DataFrame):\n",
        "            weather_row = weather_row.iloc[0]\n",
        "        # Combine both rows into a single Series with no duplicate columns\n",
        "        merged_row = pd.concat([flight_row, weather_row])\n",
        "    else:\n",
        "        merged_row = flight_row\n",
        "\n",
        "    merged_chunk.append(merged_row)\n",
        "\n",
        "    if len(merged_chunk) >= chunk_size or i == len(april_df) - 1:\n",
        "        out_df = pd.DataFrame(merged_chunk)\n",
        "        out_df.to_csv(f\"{output_prefix}{file_index}.csv\", index=False)\n",
        "        print(f\"Saved {output_prefix}{file_index}.csv with {len(out_df)} records.\")\n",
        "        file_index += 1\n",
        "        merged_chunk = []\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhz7GBIWDX_6",
        "outputId": "e3f5afb8-e865-4d70-ec81-33e18ae6bee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded datasets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Merging:  22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                | 500015/2265248 [03:17<21:55:43, 22.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved /content/Merge_part1.csv with 500000 records.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Merging:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 1000516/2265248 [06:13<7:36:39, 46.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved /content/Merge_part2.csv with 500000 records.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Merging:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 1500000/2265248 [09:00<6:12:07, 34.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved /content/Merge_part3.csv with 500000 records.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Merging:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 2000000/2265248 [11:52<2:10:51, 33.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved /content/Merge_part4.csv with 500000 records.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Merging: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2265248/2265248 [13:19<00:00, 2835.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved /content/Merge_part5.csv with 265248 records.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(weather_df))\n",
        "print(len(april_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6IUj8Kpga1w",
        "outputId": "4440ef35-25ec-441c-f220-65d8b6b87530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "77168\n",
            "2265248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge all chunks to create the final dataset"
      ],
      "metadata": {
        "id": "fwPkGBX9Dx-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# === Folder and pattern where your chunked files are stored ===\n",
        "folder_path = '.'  # Use \".\" if you're in the same folder\n",
        "file_pattern = os.path.join(folder_path, '/content/Merge_part*.csv')\n",
        "\n",
        "# === Grab all matching CSV file paths ===\n",
        "csv_files = sorted(glob.glob(file_pattern))  # Sort to maintain order\n",
        "\n",
        "print(f\"Found {len(csv_files)} files to merge.\")\n",
        "\n",
        "# === Read and concatenate ===\n",
        "merged_df = pd.concat((pd.read_csv(file) for file in csv_files), ignore_index=True)\n",
        "\n",
        "# === Save final merged CSV ===\n",
        "merged_df.to_csv('Merged_April_Flight_Weather.csv', index=False)\n",
        "print(\"Merged CSV saved as Merged_April_Flight_Weather.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwEKXY1YD1FZ",
        "outputId": "f363fa4e-d0f8-4ced-c4a3-2a69f15a3741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5 files to merge.\n",
            "Merged CSV saved as Merged_April_Flight_Weather.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(merged_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A31_AIUrlMZl",
        "outputId": "0f6b1b49-8e87-4a04-c227-77a7cf283eca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2265248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display NaNs count and percentage per column\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/Merged_April_Flight_Weather.csv\")\n",
        "nan_summary = df.isna().sum().to_frame(name='NaN Count')\n",
        "nan_summary['% Missing'] = (nan_summary['NaN Count'] / len(df)) * 100\n",
        "\n",
        "# Sort by percentage missing, descending\n",
        "nan_summary = nan_summary[nan_summary['NaN Count'] > 0].sort_values(by='% Missing', ascending=False)\n",
        "\n",
        "print(nan_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJRd8upTw6qT",
        "outputId": "de88dcc2-5bc3-4d7f-9e24-a9ff9b0485aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           NaN Count  % Missing\n",
            "WEATHER_TEMP                   90522   3.996119\n",
            "WEATHER_DEWP                   90522   3.996119\n",
            "WEATHER_SLP                    90522   3.996119\n",
            "WEATHER_VISIB                  90522   3.996119\n",
            "WEATHER_GUST                   90522   3.996119\n",
            "WEATHER_PRCP                   90522   3.996119\n",
            "WEATHER_FRSHTT                 90522   3.996119\n",
            "WEATHER_WEATHER_CONDITION      90522   3.996119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop columns with more than 30% missing data"
      ],
      "metadata": {
        "id": "PWYsr5ntyypl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of columns to drop\n",
        "columns_to_drop = [\n",
        "    'CANCELLATION_CODE'\n",
        "]\n",
        "\n",
        "# Drop the columns from the DataFrame\n",
        "df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
        "\n",
        "# Confirm\n",
        "print(f\"Dropped {len(columns_to_drop)} columns. Remaining columns: {df.shape[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwJ6vv2jxyrc",
        "outputId": "4f2e82f2-9966-4ea8-b9e6-4b1693c9fb12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropped 1 columns. Remaining columns: 28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handle missing values"
      ],
      "metadata": {
        "id": "QVexZ8XDzljN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[:, 'ARR_DEL15'] = df['ARR_DEL15'].fillna(df['ARR_DEL15'].mode()[0])\n",
        "df_cleaned = df.dropna()"
      ],
      "metadata": {
        "id": "XY2otoQ76op7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nan_summary = df_cleaned.isna().sum().to_frame(name='NaN Count')\n",
        "nan_summary['% Missing'] = (nan_summary['NaN Count'] / len(df_cleaned)) * 100\n",
        "\n",
        "# Sort by percentage missing, descending\n",
        "nan_summary = nan_summary[nan_summary['NaN Count'] > 0].sort_values(by='% Missing', ascending=False)\n",
        "\n",
        "print(nan_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDV9rA7o3p1Z",
        "outputId": "e5568831-502d-4b5b-e5ce-adb67036396c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [NaN Count, % Missing]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA for data reduction"
      ],
      "metadata": {
        "id": "YHb0hnAry48q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Select numeric columns, excluding 'ACTUAL_ELAPSED_TIME'\n",
        "numeric_cols = df_cleaned.select_dtypes(include=['int64', 'float64']).columns\n",
        "numeric_cols = [col for col in numeric_cols if col != 'ACTUAL_ELAPSED_TIME']\n",
        "\n",
        "# Create numeric_df with only the selected columns\n",
        "numeric_df = df_cleaned[numeric_cols]\n",
        "\n",
        "# Step 2: Fill missing values with column mean\n",
        "numeric_df_filled = numeric_df.fillna(numeric_df.mean())\n",
        "\n",
        "# Step 3: Correlation Analysis\n",
        "corr_matrix = numeric_df_filled.corr().abs()\n",
        "\n",
        "# Get all strongly correlated pairs (> 0.9)\n",
        "corr_pairs = corr_matrix.unstack().sort_values(ascending=False)\n",
        "corr_pairs = corr_pairs[corr_pairs < 1]  # remove self-correlation\n",
        "strong_corr = corr_pairs[corr_pairs > 0.8]\n",
        "\n",
        "print(\"Highly Correlated Pairs (Correlation > 0.8):\\n\")\n",
        "print(strong_corr)\n",
        "\n",
        "# Step 4: Standardize features\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(numeric_df_filled)\n",
        "\n",
        "# Step 5: PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(scaled_data)\n",
        "\n",
        "# Step 6: PCA Loadings\n",
        "pca_loadings = pd.DataFrame(pca.components_, columns=numeric_df.columns, index=['PC1', 'PC2'])\n",
        "\n",
        "# Show top 5 absolute contributors to PC1\n",
        "top_5_pc1 = pca_loadings.loc['PC1'].abs().sort_values(ascending=False).head(5)\n",
        "print(\"\\nTop 5 PCA Features (PC1):\\n\")\n",
        "print(top_5_pc1)\n",
        "\n",
        "# Optional: PCA result as DataFrame\n",
        "pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n",
        "print(\"\\nPCA Transformed Sample:\\n\")\n",
        "print(pca_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OLydXC6lUUT",
        "outputId": "30ff7de4-8c71-4399-8626-ea74eb51e3ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Highly Correlated Pairs (Correlation > 0.8):\n",
            "\n",
            "DISTANCE_GROUP    DISTANCE            0.987137\n",
            "DISTANCE          DISTANCE_GROUP      0.987137\n",
            "CRS_ELAPSED_TIME  DISTANCE            0.984659\n",
            "DISTANCE          CRS_ELAPSED_TIME    0.984659\n",
            "DEP_TIME          WHEELS_OFF          0.974512\n",
            "WHEELS_OFF        DEP_TIME            0.974512\n",
            "DISTANCE_GROUP    CRS_ELAPSED_TIME    0.973226\n",
            "CRS_ELAPSED_TIME  DISTANCE_GROUP      0.973226\n",
            "dtype: float64\n",
            "\n",
            "Top 5 PCA Features (PC1):\n",
            "\n",
            "DISTANCE            0.565487\n",
            "CRS_ELAPSED_TIME    0.565421\n",
            "DISTANCE_GROUP      0.563209\n",
            "TAXI_OUT            0.170359\n",
            "ARR_DEL15           0.087622\n",
            "Name: PC1, dtype: float64\n",
            "\n",
            "PCA Transformed Sample:\n",
            "\n",
            "        PC1       PC2\n",
            "0 -0.421470 -0.188980\n",
            "1 -0.431947 -0.785378\n",
            "2 -0.421709  0.391575\n",
            "3 -0.846308 -0.700878\n",
            "4 -1.545009 -0.928317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Helper: Find groups of mutually correlated features\n",
        "def build_corr_buckets(corr_matrix, threshold=0.8):\n",
        "    visited = set()\n",
        "    buckets = []\n",
        "\n",
        "    for col in corr_matrix.columns:\n",
        "        if col not in visited:\n",
        "            # Find all features correlated to this one above the threshold\n",
        "            correlated = set(corr_matrix[col][corr_matrix[col] > threshold].index)\n",
        "            correlated.add(col)\n",
        "\n",
        "            # Check if it's a new group\n",
        "            if correlated not in buckets:\n",
        "                buckets.append(correlated)\n",
        "                visited.update(correlated)\n",
        "    return buckets\n",
        "\n",
        "# Generate buckets\n",
        "corr_buckets = build_corr_buckets(corr_matrix, threshold=0.8)\n",
        "\n",
        "# Print the buckets\n",
        "print(\"\\nHighly Correlated Column Buckets (Threshold > 0.8):\\n\")\n",
        "for i, group in enumerate(corr_buckets, 1):\n",
        "    if len(group) > 1:  # Ignore singletons\n",
        "        print(f\"Group {i}: {sorted(group)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uORZYBvHw-ey",
        "outputId": "67716c61-f698-4278-8953-0aa9393b4c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Highly Correlated Column Buckets (Threshold > 0.8):\n",
            "\n",
            "Group 4: ['DEP_TIME', 'WHEELS_OFF']\n",
            "Group 11: ['CRS_ELAPSED_TIME', 'DISTANCE', 'DISTANCE_GROUP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping redundant columns from PCA\n",
        "\n",
        "# From DISTANCE, DISTANCE_GROUP, CRS_ELAPSED_TIME, and ACTUAL_ELAPSED_TIME, keep only DISTANCE_GROUP.\n",
        "df = df.drop(columns=['DISTANCE', 'CRS_ELAPSED_TIME'])\n",
        "\n",
        "# Between DEP_TIME and WHEELS_OFF, keep only DEP_TIME\n",
        "df = df.drop(columns=['WHEELS_OFF'])"
      ],
      "metadata": {
        "id": "VMkiQyo54cQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vE__i_Dj5bgi",
        "outputId": "ae7e4041-675d-4a9a-ddd8-e56d980ef342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   YEAR  DAY_OF_MONTH  DAY_OF_WEEK OP_UNIQUE_CARRIER ORIGIN ORIGIN_STATE_ABR  \\\n",
            "0  1998             1            3                UA    ABE               PA   \n",
            "1  1998             1            3                UA    ABE               PA   \n",
            "2  1998             1            3                UA    ABE               PA   \n",
            "3  1998             1            3                US    ABE               PA   \n",
            "4  1998             1            3                US    ABE               PA   \n",
            "\n",
            "  DEST DEST_STATE_ABR  DEP_TIME  DEP_DEL15  ...  DISTANCE_GROUP  DDMMYYYYIATA  \\\n",
            "0  ORD             IL    1309.0        0.0  ...               3   01041998ABE   \n",
            "1  ORD             IL     919.0        0.0  ...               3   01041998ABE   \n",
            "2  ORD             IL    1719.0        0.0  ...               3   01041998ABE   \n",
            "3  CLT             NC     819.0        0.0  ...               2   01041998ABE   \n",
            "4  PIT             PA     729.0        0.0  ...               2   01041998ABE   \n",
            "\n",
            "   WEATHER_TEMP  WEATHER_DEWP  WEATHER_SLP  WEATHER_VISIB WEATHER_GUST  \\\n",
            "0          68.7          61.0       1008.8            8.8         21.0   \n",
            "1          68.7          61.0       1008.8            8.8         21.0   \n",
            "2          68.7          61.0       1008.8            8.8         21.0   \n",
            "3          68.7          61.0       1008.8            8.8         21.0   \n",
            "4          68.7          61.0       1008.8            8.8         21.0   \n",
            "\n",
            "   WEATHER_PRCP  WEATHER_FRSHTT  WEATHER_WEATHER_CONDITION  \n",
            "0           0.0        111110.0                     stormy  \n",
            "1           0.0        111110.0                     stormy  \n",
            "2           0.0        111110.0                     stormy  \n",
            "3           0.0        111110.0                     stormy  \n",
            "4           0.0        111110.0                     stormy  \n",
            "\n",
            "[5 rows x 25 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"/content/Final_.csv\", index=False)"
      ],
      "metadata": {
        "id": "ZXGQFmIJylGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Identify numerical and categorical columns after dropping columns\n",
        "numeric_features = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Ensure 'ARR_DEL15' is not in your feature lists (since it's the target)\n",
        "numeric_features = [f for f in numeric_features if f != 'ARR_DEL15']\n",
        "categorical_features = [f for f in categorical_features if f != 'ARR_DEL15']\n",
        "\n",
        "# Define features and target\n",
        "target_column = 'ARR_DEL15'\n",
        "y = df[target_column]\n",
        "X = df.drop(columns=[target_column])\n",
        "\n",
        "# ColumnTransformer: Scaling numeric + encoding categorical\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline([\n",
        "            ('scaler', StandardScaler()),   # Standardize\n",
        "        ]), numeric_features),\n",
        "        ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Full pipeline with XGBoost\n",
        "pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('model', XGBClassifier(\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss',\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7rZ5xNZ4g2P",
        "outputId": "d7e8e19f-115e-4cc2-e31d-c2bf7420518c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:37:21] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy Score: 0.9685948229750244\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.98    372253\n",
            "         1.0       0.94      0.90      0.92     89518\n",
            "\n",
            "    accuracy                           0.97    461771\n",
            "   macro avg       0.96      0.94      0.95    461771\n",
            "weighted avg       0.97      0.97      0.97    461771\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[366865   5388]\n",
            " [  9114  80404]]\n"
          ]
        }
      ]
    }
  ]
}